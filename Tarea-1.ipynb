{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF-395 Redes Neuronales Artificiales\n",
    "## Tarea 1 - Redes Neuro\n",
    "\n",
    "### Integrantes:\n",
    "* Ignacio Valenzuelipico\n",
    "* Rodrigo González Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de proceder a realizar el trabajo importamos las librerías y paquetes que usaremos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ipresnya/aidPython/lib64/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predicción de Entalpía de Atomización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.a. Construir Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16242 entries, 0 to 16241\n",
      "Columns: 1278 entries, Unnamed: 0 to Eat\n",
      "dtypes: float64(1276), int64(2)\n",
      "memory usage: 158.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1267</th>\n",
       "      <th>1268</th>\n",
       "      <th>1269</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>pubchem_id</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "      <td>16242.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8139.041805</td>\n",
       "      <td>115.715266</td>\n",
       "      <td>22.445723</td>\n",
       "      <td>20.474191</td>\n",
       "      <td>18.529573</td>\n",
       "      <td>17.169350</td>\n",
       "      <td>15.816888</td>\n",
       "      <td>15.133152</td>\n",
       "      <td>14.471534</td>\n",
       "      <td>13.960759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>33107.484300</td>\n",
       "      <td>-11.178969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4698.182820</td>\n",
       "      <td>113.198503</td>\n",
       "      <td>8.659586</td>\n",
       "      <td>7.670481</td>\n",
       "      <td>6.485777</td>\n",
       "      <td>5.512560</td>\n",
       "      <td>4.179691</td>\n",
       "      <td>3.885091</td>\n",
       "      <td>3.503075</td>\n",
       "      <td>3.357136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.043869</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.024472</td>\n",
       "      <td>23456.785147</td>\n",
       "      <td>3.659133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.858105</td>\n",
       "      <td>2.906146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-23.245373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4068.250000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>17.969345</td>\n",
       "      <td>16.228071</td>\n",
       "      <td>15.165862</td>\n",
       "      <td>13.744092</td>\n",
       "      <td>13.653146</td>\n",
       "      <td>13.637784</td>\n",
       "      <td>12.759519</td>\n",
       "      <td>12.587359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12298.250000</td>\n",
       "      <td>-13.475805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8142.500000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>20.662511</td>\n",
       "      <td>18.631287</td>\n",
       "      <td>17.690729</td>\n",
       "      <td>16.020040</td>\n",
       "      <td>15.156646</td>\n",
       "      <td>13.848274</td>\n",
       "      <td>13.659233</td>\n",
       "      <td>13.652832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27731.500000</td>\n",
       "      <td>-10.835211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12207.750000</td>\n",
       "      <td>73.516695</td>\n",
       "      <td>21.132432</td>\n",
       "      <td>20.739496</td>\n",
       "      <td>18.712895</td>\n",
       "      <td>18.297501</td>\n",
       "      <td>17.639688</td>\n",
       "      <td>16.154918</td>\n",
       "      <td>15.499474</td>\n",
       "      <td>14.900585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55020.750000</td>\n",
       "      <td>-8.623903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16272.000000</td>\n",
       "      <td>388.023441</td>\n",
       "      <td>73.563510</td>\n",
       "      <td>66.269180</td>\n",
       "      <td>66.268891</td>\n",
       "      <td>66.268756</td>\n",
       "      <td>66.268196</td>\n",
       "      <td>66.264158</td>\n",
       "      <td>66.258487</td>\n",
       "      <td>66.258177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062225</td>\n",
       "      <td>0.061999</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.061534</td>\n",
       "      <td>0.059760</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.057834</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>74980.000000</td>\n",
       "      <td>-0.789513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0             0             1             2             3  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean    8139.041805    115.715266     22.445723     20.474191     18.529573   \n",
       "std     4698.182820    113.198503      8.659586      7.670481      6.485777   \n",
       "min        0.000000     36.858105      2.906146      0.000000      0.000000   \n",
       "25%     4068.250000     73.516695     17.969345     16.228071     15.165862   \n",
       "50%     8142.500000     73.516695     20.662511     18.631287     17.690729   \n",
       "75%    12207.750000     73.516695     21.132432     20.739496     18.712895   \n",
       "max    16272.000000    388.023441     73.563510     66.269180     66.268891   \n",
       "\n",
       "                  4             5             6             7             8  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean      17.169350     15.816888     15.133152     14.471534     13.960759   \n",
       "std        5.512560      4.179691      3.885091      3.503075      3.357136   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       13.744092     13.653146     13.637784     12.759519     12.587359   \n",
       "50%       16.020040     15.156646     13.848274     13.659233     13.652832   \n",
       "75%       18.297501     17.639688     16.154918     15.499474     14.900585   \n",
       "max       66.268756     66.268196     66.264158     66.258487     66.258177   \n",
       "\n",
       "           ...               1267          1268          1269          1270  \\\n",
       "count      ...       16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       ...           0.000134      0.000133      0.003879      0.000131   \n",
       "std        ...           0.002728      0.002705      0.043869      0.002676   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "max        ...           0.062225      0.061999      0.500000      0.061534   \n",
       "\n",
       "               1271          1272          1273          1274    pubchem_id  \\\n",
       "count  16242.000000  16242.000000  16242.000000  16242.000000  16242.000000   \n",
       "mean       0.000129      0.002155      0.000127      0.001201  33107.484300   \n",
       "std        0.002633      0.032755      0.002594      0.024472  23456.785147   \n",
       "min        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000  12298.250000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000  27731.500000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000  55020.750000   \n",
       "max        0.059760      0.500000      0.057834      0.500000  74980.000000   \n",
       "\n",
       "                Eat  \n",
       "count  16242.000000  \n",
       "mean     -11.178969  \n",
       "std        3.659133  \n",
       "min      -23.245373  \n",
       "25%      -13.475805  \n",
       "50%      -10.835211  \n",
       "75%       -8.623903  \n",
       "max       -0.789513  \n",
       "\n",
       "[8 rows x 1278 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos= pd.read_csv(\"roboBohr.csv\")\n",
    "datos.shape\n",
    "datos.info()\n",
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.drop(columns=['Unnamed: 0','pubchem_id'],axis=1,inplace=True)\n",
    "total=len(datos)\n",
    "df_train=datos[:int(0.6*total)]                       #60% de los datos\n",
    "df_val=datos[int(0.6*total):int(0.85*total)]          #25% de los datos\n",
    "df_test=datos[int(0.85*total)::]                      #15% restante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a.1. Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "X_val_scaled =  pd.DataFrame(scaler.transform(df_val),columns=df_val.columns)\n",
    "X_test_scaled =  pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.pop('Eat').values.reshape(-1,1)\n",
    "y_val = df_val.pop('Eat').values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_val_scaled.drop(columns=['Eat'],axis=1,inplace=True)\n",
    "X_test_scaled.drop(columns=['Eat'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b.  Red feedfoward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.matrix(X_train_scaled)\n",
    "a_val_scaled = np.matrix(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9745 samples, validate on 4060 samples\n",
      "Epoch 1/250\n",
      "9745/9745 [==============================] - 4s 364us/step - loss: 1.3641 - val_loss: 0.5621\n",
      "Epoch 2/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.6090 - val_loss: 0.8801\n",
      "Epoch 3/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.5136 - val_loss: 0.3744\n",
      "Epoch 4/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.4310 - val_loss: 0.3257\n",
      "Epoch 5/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.3795 - val_loss: 0.4543\n",
      "Epoch 6/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.3246 - val_loss: 0.2627\n",
      "Epoch 7/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.2836 - val_loss: 0.2383\n",
      "Epoch 8/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.2556 - val_loss: 0.2151\n",
      "Epoch 9/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.2275 - val_loss: 0.1920\n",
      "Epoch 10/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.2014 - val_loss: 0.1941\n",
      "Epoch 11/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.1801 - val_loss: 0.1619\n",
      "Epoch 12/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.1639 - val_loss: 0.2014\n",
      "Epoch 13/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.1449 - val_loss: 0.1514\n",
      "Epoch 14/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.1344 - val_loss: 0.1330\n",
      "Epoch 15/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.1197 - val_loss: 0.1560\n",
      "Epoch 16/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.1120 - val_loss: 0.1583\n",
      "Epoch 17/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.1043 - val_loss: 0.1155\n",
      "Epoch 18/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0994 - val_loss: 0.1001\n",
      "Epoch 19/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0907 - val_loss: 0.1786\n",
      "Epoch 20/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0866 - val_loss: 0.1242\n",
      "Epoch 21/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0811 - val_loss: 0.1002\n",
      "Epoch 22/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0786 - val_loss: 0.0844\n",
      "Epoch 23/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0741 - val_loss: 0.0928\n",
      "Epoch 24/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0714 - val_loss: 0.0788\n",
      "Epoch 25/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0694 - val_loss: 0.1266\n",
      "Epoch 26/250\n",
      "9745/9745 [==============================] - 3s 268us/step - loss: 0.0677 - val_loss: 0.0802\n",
      "Epoch 27/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0644 - val_loss: 0.0725\n",
      "Epoch 28/250\n",
      "9745/9745 [==============================] - 3s 280us/step - loss: 0.0639 - val_loss: 0.0747\n",
      "Epoch 29/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0620 - val_loss: 0.0750\n",
      "Epoch 30/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0602 - val_loss: 0.0776\n",
      "Epoch 31/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0571 - val_loss: 0.0931\n",
      "Epoch 32/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0564 - val_loss: 0.0766\n",
      "Epoch 33/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0557 - val_loss: 0.0918\n",
      "Epoch 34/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0526 - val_loss: 0.0784\n",
      "Epoch 35/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0530 - val_loss: 0.0664\n",
      "Epoch 36/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0514 - val_loss: 0.0645\n",
      "Epoch 37/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0514 - val_loss: 0.0645\n",
      "Epoch 38/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0490 - val_loss: 0.0631\n",
      "Epoch 39/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0475 - val_loss: 0.0653\n",
      "Epoch 40/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0481 - val_loss: 0.0672\n",
      "Epoch 41/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0478 - val_loss: 0.0670\n",
      "Epoch 42/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0457 - val_loss: 0.0578\n",
      "Epoch 43/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0443 - val_loss: 0.0570\n",
      "Epoch 44/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0443 - val_loss: 0.0885\n",
      "Epoch 45/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0436 - val_loss: 0.0575\n",
      "Epoch 46/250\n",
      "9745/9745 [==============================] - 3s 267us/step - loss: 0.0437 - val_loss: 0.0569\n",
      "Epoch 47/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0419 - val_loss: 0.0686\n",
      "Epoch 48/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0423 - val_loss: 0.0579\n",
      "Epoch 49/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.0411 - val_loss: 0.0560\n",
      "Epoch 50/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0399 - val_loss: 0.0556\n",
      "Epoch 51/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0407 - val_loss: 0.0534\n",
      "Epoch 52/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0393 - val_loss: 0.0548\n",
      "Epoch 53/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.0393 - val_loss: 0.0515\n",
      "Epoch 54/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0388 - val_loss: 0.0750\n",
      "Epoch 55/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0383 - val_loss: 0.0554\n",
      "Epoch 56/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0378 - val_loss: 0.0536\n",
      "Epoch 57/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0370 - val_loss: 0.0551\n",
      "Epoch 58/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0365 - val_loss: 0.0909\n",
      "Epoch 59/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0370 - val_loss: 0.0547\n",
      "Epoch 60/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0354 - val_loss: 0.0525\n",
      "Epoch 61/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0351 - val_loss: 0.1406\n",
      "Epoch 62/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0353 - val_loss: 0.0517\n",
      "Epoch 63/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0347 - val_loss: 0.0519\n",
      "Epoch 64/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0359 - val_loss: 0.0611\n",
      "Epoch 65/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0341 - val_loss: 0.0521\n",
      "Epoch 66/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0343 - val_loss: 0.0619\n",
      "Epoch 67/250\n",
      "9745/9745 [==============================] - 2s 240us/step - loss: 0.0341 - val_loss: 0.0521\n",
      "Epoch 68/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0332 - val_loss: 0.0634\n",
      "Epoch 69/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0324 - val_loss: 0.0481\n",
      "Epoch 70/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0320 - val_loss: 0.0532\n",
      "Epoch 71/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.0315 - val_loss: 0.0581\n",
      "Epoch 72/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.0333 - val_loss: 0.0564\n",
      "Epoch 73/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0313 - val_loss: 0.0583\n",
      "Epoch 74/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0311 - val_loss: 0.0514\n",
      "Epoch 75/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0313 - val_loss: 0.0553\n",
      "Epoch 76/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0303 - val_loss: 0.0552\n",
      "Epoch 77/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0317 - val_loss: 0.0498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0308 - val_loss: 0.0702\n",
      "Epoch 79/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0298 - val_loss: 0.0615\n",
      "Epoch 80/250\n",
      "9745/9745 [==============================] - 3s 298us/step - loss: 0.0293 - val_loss: 0.0508\n",
      "Epoch 81/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0297 - val_loss: 0.0484\n",
      "Epoch 82/250\n",
      "9745/9745 [==============================] - 3s 266us/step - loss: 0.0289 - val_loss: 0.0473\n",
      "Epoch 83/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0291 - val_loss: 0.0736\n",
      "Epoch 84/250\n",
      "9745/9745 [==============================] - 3s 258us/step - loss: 0.0292 - val_loss: 0.0491\n",
      "Epoch 85/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0296 - val_loss: 0.0465\n",
      "Epoch 86/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0283 - val_loss: 0.0463\n",
      "Epoch 87/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0288 - val_loss: 0.0455\n",
      "Epoch 88/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0287 - val_loss: 0.0651\n",
      "Epoch 89/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0281 - val_loss: 0.0532\n",
      "Epoch 90/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0288 - val_loss: 0.0694\n",
      "Epoch 91/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0286 - val_loss: 0.0756\n",
      "Epoch 92/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0270 - val_loss: 0.0577\n",
      "Epoch 93/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0275 - val_loss: 0.0468\n",
      "Epoch 94/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.0274 - val_loss: 0.0495\n",
      "Epoch 95/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0272 - val_loss: 0.0544\n",
      "Epoch 96/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0269 - val_loss: 0.0431\n",
      "Epoch 97/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.0267 - val_loss: 0.0447\n",
      "Epoch 98/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0264 - val_loss: 0.0454\n",
      "Epoch 99/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0261 - val_loss: 0.0482\n",
      "Epoch 100/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0263 - val_loss: 0.0505\n",
      "Epoch 101/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0267 - val_loss: 0.0517\n",
      "Epoch 102/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0264 - val_loss: 0.0564\n",
      "Epoch 103/250\n",
      "9745/9745 [==============================] - 2s 242us/step - loss: 0.0260 - val_loss: 0.0458\n",
      "Epoch 104/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0254 - val_loss: 0.0439\n",
      "Epoch 105/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0255 - val_loss: 0.0453\n",
      "Epoch 106/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0251 - val_loss: 0.0473\n",
      "Epoch 107/250\n",
      "9745/9745 [==============================] - 3s 262us/step - loss: 0.0244 - val_loss: 0.0421\n",
      "Epoch 108/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0250 - val_loss: 0.0532\n",
      "Epoch 109/250\n",
      "9745/9745 [==============================] - 3s 297us/step - loss: 0.0246 - val_loss: 0.0405\n",
      "Epoch 110/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0247 - val_loss: 0.0421\n",
      "Epoch 111/250\n",
      "9745/9745 [==============================] - 2s 254us/step - loss: 0.0254 - val_loss: 0.0425\n",
      "Epoch 112/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0244 - val_loss: 0.0406\n",
      "Epoch 113/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0241 - val_loss: 0.0415\n",
      "Epoch 114/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0242 - val_loss: 0.0554\n",
      "Epoch 115/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0236 - val_loss: 0.0809\n",
      "Epoch 116/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0233 - val_loss: 0.0420\n",
      "Epoch 117/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0243 - val_loss: 0.0641\n",
      "Epoch 118/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0242 - val_loss: 0.0450\n",
      "Epoch 119/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0240 - val_loss: 0.0441\n",
      "Epoch 120/250\n",
      "9745/9745 [==============================] - 3s 269us/step - loss: 0.0231 - val_loss: 0.0432\n",
      "Epoch 121/250\n",
      "9745/9745 [==============================] - 3s 270us/step - loss: 0.0228 - val_loss: 0.0409\n",
      "Epoch 122/250\n",
      "9745/9745 [==============================] - 3s 279us/step - loss: 0.0239 - val_loss: 0.0436\n",
      "Epoch 123/250\n",
      "9745/9745 [==============================] - 3s 278us/step - loss: 0.0234 - val_loss: 0.0479\n",
      "Epoch 124/250\n",
      "9745/9745 [==============================] - 3s 276us/step - loss: 0.0234 - val_loss: 0.0601\n",
      "Epoch 125/250\n",
      "9745/9745 [==============================] - 3s 317us/step - loss: 0.0223 - val_loss: 0.0727\n",
      "Epoch 126/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0237 - val_loss: 0.0511\n",
      "Epoch 127/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0223 - val_loss: 0.0466\n",
      "Epoch 128/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0224 - val_loss: 0.0487\n",
      "Epoch 129/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0224 - val_loss: 0.0415\n",
      "Epoch 130/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0220 - val_loss: 0.0435\n",
      "Epoch 131/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0217 - val_loss: 0.0497\n",
      "Epoch 132/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0220 - val_loss: 0.0403\n",
      "Epoch 133/250\n",
      "9745/9745 [==============================] - 3s 288us/step - loss: 0.0212 - val_loss: 0.0393\n",
      "Epoch 134/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0219 - val_loss: 0.0392\n",
      "Epoch 135/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.0218 - val_loss: 0.0581\n",
      "Epoch 136/250\n",
      "9745/9745 [==============================] - 3s 264us/step - loss: 0.0209 - val_loss: 0.0400\n",
      "Epoch 137/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0211 - val_loss: 0.0484\n",
      "Epoch 138/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0207 - val_loss: 0.0412\n",
      "Epoch 139/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0213 - val_loss: 0.0420\n",
      "Epoch 140/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0210 - val_loss: 0.0454\n",
      "Epoch 141/250\n",
      "9745/9745 [==============================] - 2s 251us/step - loss: 0.0210 - val_loss: 0.0396\n",
      "Epoch 142/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0207 - val_loss: 0.0402\n",
      "Epoch 143/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0207 - val_loss: 0.0400\n",
      "Epoch 144/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0210 - val_loss: 0.0387\n",
      "Epoch 145/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0211 - val_loss: 0.0402\n",
      "Epoch 146/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0207 - val_loss: 0.0540\n",
      "Epoch 147/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0206 - val_loss: 0.0415\n",
      "Epoch 148/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0210 - val_loss: 0.0459\n",
      "Epoch 149/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0205 - val_loss: 0.0395\n",
      "Epoch 150/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0197 - val_loss: 0.0433\n",
      "Epoch 151/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0200 - val_loss: 0.0432\n",
      "Epoch 152/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0206 - val_loss: 0.0423\n",
      "Epoch 153/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0203 - val_loss: 0.0417\n",
      "Epoch 154/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0200 - val_loss: 0.0461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0194 - val_loss: 0.0387\n",
      "Epoch 156/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0197 - val_loss: 0.0393\n",
      "Epoch 157/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.0194 - val_loss: 0.0637\n",
      "Epoch 158/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0201 - val_loss: 0.0451\n",
      "Epoch 159/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0197 - val_loss: 0.0398\n",
      "Epoch 160/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0192 - val_loss: 0.0422\n",
      "Epoch 161/250\n",
      "9745/9745 [==============================] - 3s 273us/step - loss: 0.0192 - val_loss: 0.0422\n",
      "Epoch 162/250\n",
      "9745/9745 [==============================] - 3s 291us/step - loss: 0.0198 - val_loss: 0.0401\n",
      "Epoch 163/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.0198 - val_loss: 0.0400\n",
      "Epoch 164/250\n",
      "9745/9745 [==============================] - 2s 253us/step - loss: 0.0194 - val_loss: 0.0424\n",
      "Epoch 165/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0194 - val_loss: 0.0466\n",
      "Epoch 166/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0197 - val_loss: 0.0498\n",
      "Epoch 167/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0191 - val_loss: 0.0475\n",
      "Epoch 168/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0185 - val_loss: 0.0396\n",
      "Epoch 169/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0180 - val_loss: 0.0418\n",
      "Epoch 170/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0189 - val_loss: 0.0387\n",
      "Epoch 171/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0182 - val_loss: 0.0413\n",
      "Epoch 172/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0181 - val_loss: 0.0374\n",
      "Epoch 173/250\n",
      "9745/9745 [==============================] - 3s 259us/step - loss: 0.0187 - val_loss: 0.0374\n",
      "Epoch 174/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0182 - val_loss: 0.0420\n",
      "Epoch 175/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0181 - val_loss: 0.0440\n",
      "Epoch 176/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0186 - val_loss: 0.0467\n",
      "Epoch 177/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0186 - val_loss: 0.0382\n",
      "Epoch 178/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0180 - val_loss: 0.0365\n",
      "Epoch 179/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0181 - val_loss: 0.0384\n",
      "Epoch 180/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0186 - val_loss: 0.0522\n",
      "Epoch 181/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0177 - val_loss: 0.0381\n",
      "Epoch 182/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0174 - val_loss: 0.0385\n",
      "Epoch 183/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0173 - val_loss: 0.0367\n",
      "Epoch 184/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0178 - val_loss: 0.0359\n",
      "Epoch 185/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0175 - val_loss: 0.0371\n",
      "Epoch 186/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0179 - val_loss: 0.0368\n",
      "Epoch 187/250\n",
      "9745/9745 [==============================] - 3s 272us/step - loss: 0.0178 - val_loss: 0.0363\n",
      "Epoch 188/250\n",
      "9745/9745 [==============================] - 3s 260us/step - loss: 0.0170 - val_loss: 0.0394\n",
      "Epoch 189/250\n",
      "9745/9745 [==============================] - 3s 282us/step - loss: 0.0173 - val_loss: 0.0437\n",
      "Epoch 190/250\n",
      "9745/9745 [==============================] - 2s 252us/step - loss: 0.0176 - val_loss: 0.0401\n",
      "Epoch 191/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0169 - val_loss: 0.0398\n",
      "Epoch 192/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0173 - val_loss: 0.0461\n",
      "Epoch 193/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0172 - val_loss: 0.0393\n",
      "Epoch 194/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0172 - val_loss: 0.0506\n",
      "Epoch 195/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0172 - val_loss: 0.0356\n",
      "Epoch 196/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0168 - val_loss: 0.0408\n",
      "Epoch 197/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0172 - val_loss: 0.0390\n",
      "Epoch 198/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0163 - val_loss: 0.0366\n",
      "Epoch 199/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0162 - val_loss: 0.0393\n",
      "Epoch 200/250\n",
      "9745/9745 [==============================] - 2s 250us/step - loss: 0.0166 - val_loss: 0.0474\n",
      "Epoch 201/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0169 - val_loss: 0.0409\n",
      "Epoch 202/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0173 - val_loss: 0.0380\n",
      "Epoch 203/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0164 - val_loss: 0.0462\n",
      "Epoch 204/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0161 - val_loss: 0.0448\n",
      "Epoch 205/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0167 - val_loss: 0.0434\n",
      "Epoch 206/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0159 - val_loss: 0.0496\n",
      "Epoch 207/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0168 - val_loss: 0.0356\n",
      "Epoch 208/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0159 - val_loss: 0.0390\n",
      "Epoch 209/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0163 - val_loss: 0.0382\n",
      "Epoch 210/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0160 - val_loss: 0.0447\n",
      "Epoch 211/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0164 - val_loss: 0.0447\n",
      "Epoch 212/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0157 - val_loss: 0.0368\n",
      "Epoch 213/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0161 - val_loss: 0.0363\n",
      "Epoch 214/250\n",
      "9745/9745 [==============================] - 3s 277us/step - loss: 0.0159 - val_loss: 0.0353\n",
      "Epoch 215/250\n",
      "9745/9745 [==============================] - 3s 261us/step - loss: 0.0169 - val_loss: 0.0421\n",
      "Epoch 216/250\n",
      "9745/9745 [==============================] - 3s 283us/step - loss: 0.0159 - val_loss: 0.0440\n",
      "Epoch 217/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0168 - val_loss: 0.0378\n",
      "Epoch 218/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0155 - val_loss: 0.0409\n",
      "Epoch 219/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0165 - val_loss: 0.0388\n",
      "Epoch 220/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0159 - val_loss: 0.0370\n",
      "Epoch 221/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0159 - val_loss: 0.0370\n",
      "Epoch 222/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0162 - val_loss: 0.0394\n",
      "Epoch 223/250\n",
      "9745/9745 [==============================] - 2s 244us/step - loss: 0.0152 - val_loss: 0.0360\n",
      "Epoch 224/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0157 - val_loss: 0.0395\n",
      "Epoch 225/250\n",
      "9745/9745 [==============================] - 3s 263us/step - loss: 0.0156 - val_loss: 0.0363\n",
      "Epoch 226/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.0153 - val_loss: 0.0374\n",
      "Epoch 227/250\n",
      "9745/9745 [==============================] - 3s 290us/step - loss: 0.0154 - val_loss: 0.0370\n",
      "Epoch 228/250\n",
      "9745/9745 [==============================] - 3s 271us/step - loss: 0.0149 - val_loss: 0.0436\n",
      "Epoch 229/250\n",
      "9745/9745 [==============================] - 2s 249us/step - loss: 0.0165 - val_loss: 0.0391\n",
      "Epoch 230/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0155 - val_loss: 0.0431\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0147 - val_loss: 0.0378\n",
      "Epoch 232/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0152 - val_loss: 0.0356\n",
      "Epoch 233/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0159 - val_loss: 0.0387\n",
      "Epoch 234/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0153 - val_loss: 0.0364\n",
      "Epoch 235/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0154 - val_loss: 0.0412\n",
      "Epoch 236/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0151 - val_loss: 0.0364\n",
      "Epoch 237/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0151 - val_loss: 0.0447\n",
      "Epoch 238/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0146 - val_loss: 0.0356\n",
      "Epoch 239/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0146 - val_loss: 0.0361\n",
      "Epoch 240/250\n",
      "9745/9745 [==============================] - 2s 248us/step - loss: 0.0154 - val_loss: 0.0358\n",
      "Epoch 241/250\n",
      "9745/9745 [==============================] - 3s 303us/step - loss: 0.0146 - val_loss: 0.0386\n",
      "Epoch 242/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0146 - val_loss: 0.0458\n",
      "Epoch 243/250\n",
      "9745/9745 [==============================] - 3s 294us/step - loss: 0.0145 - val_loss: 0.0360\n",
      "Epoch 244/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0143 - val_loss: 0.0372\n",
      "Epoch 245/250\n",
      "9745/9745 [==============================] - 2s 246us/step - loss: 0.0143 - val_loss: 0.0390\n",
      "Epoch 246/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0147 - val_loss: 0.0410\n",
      "Epoch 247/250\n",
      "9745/9745 [==============================] - 2s 247us/step - loss: 0.0151 - val_loss: 0.0395\n",
      "Epoch 248/250\n",
      "9745/9745 [==============================] - 2s 245us/step - loss: 0.0147 - val_loss: 0.0420\n",
      "Epoch 249/250\n",
      "9745/9745 [==============================] - 2s 243us/step - loss: 0.0144 - val_loss: 0.0357\n",
      "Epoch 250/250\n",
      "9745/9745 [==============================] - 3s 257us/step - loss: 0.0143 - val_loss: 0.0404\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train_scaled.shape[1], kernel_initializer='uniform',activation=\"sigmoid\"))\n",
    "model.add(Dense(1, kernel_initializer='uniform',activation=\"linear\")) \n",
    "model.compile(optimizer=SGD(lr=0.01),loss='mean_squared_error')\n",
    "history = model.fit(a, y_train, epochs=250, verbose=1, validation_data=(a_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
